{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dicks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello World.', \"I'm coming home soon.\", 'Ciao']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "text=\"Hello World. I'm coming home soon. Ciao\"\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre', 'Aubameyang', '.', 'Gabon', 'Player', ',', 'will', 'join', 'Chinese', 'League', 'next', 'window']\n"
     ]
    }
   ],
   "source": [
    "text=nltk.word_tokenize(\"Pierre Aubameyang. Gabon Player, will join Chinese League next window\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Good Morning.How are you?\n",
      "Length of my sentence is 4 words\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "text=input(\"Hello, Good Morning.\")\n",
    "print(\"Length of my sentence is\", len(word_tokenize(text)),'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Have',\n",
       " 'a',\n",
       " 'great',\n",
       " 'journey',\n",
       " 'to',\n",
       " 'France.',\n",
       " 'I',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'find',\n",
       " 'France',\n",
       " 'interesting']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer= TreebankWordTokenizer()\n",
    "tokenizer.tokenize(\"Have a great journey to France. I hope you find France interesting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'hesistate', 'to', 'call', 'me']\n"
     ]
    }
   ],
   "source": [
    "text=nltk.word_tokenize(\"Don't hesistate to call me\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don', \"'\", 't', 'hesistate', 'to', 'call', 'me']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer=WordPunctTokenizer()\n",
    "tokenizer.tokenize(\"Don't hesistate to call me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Don't\", 'hesistate', 'to', 'call', 'me']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize using whitespaces\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer=RegexpTokenizer('\\s+', gaps=True)\n",
    "tokenizer.tokenize(\"Don't hesistate to call me\")                          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She', 'She', 'She', 'Bravo']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select Words starting with capital letter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "sent=\"She scored 95.6 % in class. She is a good student. She is going places. Bravo\"\n",
    "tokenizer=RegexpTokenizer('[A-Z]\\w+')\n",
    "tokenizer.tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She',\n",
       " 'scored',\n",
       " '95.6',\n",
       " '%',\n",
       " 'in',\n",
       " 'class.',\n",
       " 'She',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'student.',\n",
       " 'She',\n",
       " 'is',\n",
       " 'going',\n",
       " 'places.',\n",
       " 'Bravo']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenization of Strings using whitespace\n",
    "sent=\"She scored 95.6 % in class. She is a good student. She is going places. Bravo\"\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "WhitespaceTokenizer().tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She scored 95.6 % in class. She is a good student. She is going places. Bravo']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import BlanklineTokenizer\n",
    "sent=\"She scored 95.6 % in class. She is a good student. She is going places. Bravo\"\n",
    "BlanklineTokenizer().tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She',\n",
       " 'scored',\n",
       " '95.6',\n",
       " '%',\n",
       " 'in',\n",
       " 'class.',\n",
       " 'She',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'student.',\n",
       " 'She',\n",
       " 'is',\n",
       " 'going',\n",
       " 'places.',\n",
       " 'Bravo']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent=\"She scored 95.6 % in class. She is a good student. She is going places. Bravo\"\n",
    "sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HARDWORK IS THE KEY TO SUCCESS\n"
     ]
    }
   ],
   "source": [
    "text=\"Hardwork is the key to success\"\n",
    "print(text.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She scored 95.6 % in class. She is a good student. She is going places. Bravo']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent=\"She scored 95.6 % in class. She is a good student. She is going places. Bravo\"\n",
    "sent.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['It', 'is', 'a', 'pleasant', 'evening', 'Guests', 'arrived', 'at', '8am'], ['It', 'is', 'a', 'pleasant', 'evening', 'Guests', 'arrived', 'at', '8am'], ['It', 'is', 'a', 'pleasant', 'evening', 'Guests', 'arrived', 'at', '8am'], ['It', 'is', 'a', 'pleasant', 'evening', 'Guests', 'arrived', 'at', '8am'], ['It', 'is', 'a', 'pleasant', 'evening', 'Guests', 'arrived', 'at', '8am'], ['It', 'is', 'a', 'pleasant', 'evening', 'Guests', 'arrived', 'at', '8am'], ['It', 'is', 'a', 'pleasant', 'evening', 'Guests', 'arrived', 'at', '8am'], ['It', 'is', 'a', 'pleasant', 'evening', 'Guests', 'arrived', 'at', '8am'], ['It', 'is', 'a', 'pleasant', 'evening', 'Guests', 'arrived', 'at', '8am'], ['It', 'is', 'a', 'pleasant', 'evening', 'Guests', 'arrived', 'at', '8am'], ['It', 'is', 'a', 'pleasant', 'evening', 'Guests', 'arrived', 'at', '8am'], ['They', 'came', 'from', 'the', 'US'], ['They', 'came', 'from', 'the', 'US'], ['They', 'came', 'from', 'the', 'US'], ['They', 'came', 'from', 'the', 'US'], ['They', 'came', 'from', 'the', 'US'], ['Food', 'was', 'tasty'], ['Food', 'was', 'tasty'], ['Food', 'was', 'tasty']]\n"
     ]
    }
   ],
   "source": [
    "#Removing Punctuations\n",
    "import re \n",
    "import string\n",
    "text=[\"It is a pleasant evening. Guests arrived at 8am.\",\"They  came from the US\",\"Food was tasty\" ]\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenize_docs=[word_tokenize(docs) for docs in text]\n",
    "x=re.compile('[%s]'%re.escape(string.punctuation))\n",
    "tokenized_docs_no_punctuation=[]\n",
    "for review in tokenize_docs:\n",
    "    new_review=[]\n",
    "    for token in review:\n",
    "        new_token=x.sub(u'', token)\n",
    "        if not new_token==u'':\n",
    "            new_review.append(new_token)\n",
    "        tokenized_docs_no_punctuation.append(new_review)\n",
    "print(tokenized_docs_no_punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dicks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Don't\", 'hesitate', 'contact me']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stops=set(stopwords.words('english'))\n",
    "words=[\"Don't\" ,'hesitate' ,'to' ,'contact me']\n",
    "[word for word in words if word not in stops]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "replacement_patterns=[(r'won\\'t', 'will not') ,(r'can\\'t','cannot'), (r'i\\'m', 'i am'), (r'ain\\'t','is not'),(r'(\\w+)\\'ll','\\g<1> will'),(r'(\\w+)n\\'t', '\\g<1> not'),(r'(\\w+)\\'ve','\\g<1> have'),(r'(\\w+)\\'s','\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'),(r'(\\w+)\\'d','g<1> would')]\n",
    "class RegexpReplacer(object):\n",
    "    def _init_(self, patterns=replacement_patterns):\n",
    "        self.patterns=[(re.compile(regex), repl) for (regex, repl)\n",
    "                       in  patterns]\n",
    "    def replace(self, text):\n",
    "        s=text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            (s,count)=re.subn(pattern, repl,s)\n",
    "        \n",
    "        return s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project 2Inferential Statistics: Hypothesis TestingDickson WanjauALY6015 81058 Intermediate Analytics SPRING 2019 CPSInstructor: Dr. Roseanna HopperDue Date: April 24, 2019.IntroductionHypothesis testing examines whether a hypothesis about a given population is true by coming up with a pair of opposing hypotheses. The first is called the null hypothesis (H0). The null hypothesis means no effect, or no change was observed in the population that cannot be explained by random chance. The second hypothesis is known as the alternative hypothesis (Ha). It states that any change seen in the population was too improbable to be explained by random chance. (2018). Using Hypothesis Testing in Business. Retrieved from: .  The  is to either fail to reject the null hypothesis or reject it. In our below section we perform the one-sample t-test, the two-sample t-test, the paired t-test, test of equal variation and the F- test on some preloaded datasets in R.AnalysisPart AFor the initial part, we use the “chem” dataset in R to test the hypothesis that the flour production company is producing whole meal flour with greater than one part per million copper in it.First,we load the requisite packages and data into R using the following codes:>library(MASS)> require(datasets)> data(\"chem\")We define the null and alternative hypothesis as follows:H0: Uc ≤ 1 \n",
      "\n",
      "H1: Uc > 1                  where Uc is the sample mean and 1 is the hypothesized meanTherefore, we have a directional (one-tailed) hypothesis problem.We perform the one-sample t-test using the R function t.test() as follows:> t.test(chem, mu = 1, alternative = \"greater\")\tOne Sample t-testdata:  chemt = 3.0337, df = 23, p-value = 0.002952alternative hypothesis: true mean is greater than 195 percent confidence interval: 2.427162      Infsample estimates:mean of x  4.280417 From the results t is the t-test statistic value (t = 3.0337)df is the degrees of freedom (df= 23)p-value  (p-value= 0.002952)sample estimates is the mean value of the sample (mean = 4.280417)Since the P-value of the test (0.002952) is less than the level of significance alpha=0.05. We reject our null hypothesis and conclude that the flour production company is producing whole meal flour with greater than one part per million copper in it.Next, we load up the “cats” dataset into R and use it to answer the question: Do male and female cat samples have the same body weight?> data(\"cats\")> catsWe define the null and alternative hypothesis as follows:H0: Um = Uf\n",
      "\n",
      "H1: Um ≠ Uf                      where Uf   is the mean of female cats and Um mean of male catsFirst, we subset the female and male cats from the dataframe as follows:> female<-subset(cats, cats$Sex==\"F\")> male<-subset(cats, cats$Sex==\"M\")Then we proceed to perform the two-sample t-test in R using the code:cat<- t.test(male$Bwt, female$Bwt, alternative = \"two.sided\", var.equal = TRUE)> cat\tTwo Sample t-testdata:  male$Bwt and female$Bwtt = 7.3307, df = 142, p-value = 1.59e-11alternative hypothesis: true difference in means is not equal to 095 percent confidence interval: 0.3946927 0.6861584sample estimates:mean of x mean of y  2.900000  2.359574 From the results, t is the t-test statistic value (t = 7.3307)df is the degrees of freedom (df= 142)p-value (p-value= 1.59e-11)sample estimates are the mean value of the samples for x=males, y=females (mean = 2.9, 2.359574)Since the P-value of the test (1.59e-11) is way less than the level of significance alpha=0.05. We reject our null hypothesis and conclude that male cats’ body weight is significantly different from female cats’ body weight with a p-value = 1.59e-11. Next, we perform a paired t-test on the “shoes” dataset in R to answer the question: “did material A wear better than material B?”The paired t-test is used to compare the means of two related groups of sample, in our case, shoes sample A and B.We define our null and alternative hypothesis as follows:H0:UA≤UB\n",
      "\n",
      "H1: UA > UB                    where Uf   is the mean of material A and Um mean of material BWe have a directional (one-tailed) hypothesis problem.We load the data set into R and apply the paired t-test in R as follows:> t.test(shoes$A, shoes$B, paired = TRUE, alternative = \"greater\")\tPaired t-testdata:  shoes$A and shoes$Bt = -3.3489, df = 9, p-value = 0.9957alternative hypothesis: true difference in means is greater than 095 percent confidence interval: -0.6344264        Infsample estimates:mean of the differences                   -0.41 From the above results:t is the t-test statistic value (t = -3.3489)df is the degrees of freedom (df= 9)p-value (p-value= 0.9957)sample estimates is the mean differences between the pairs(mean =-0.41)The p- value of 0.9957 is significantly greater than our level of significance of 0.05. We fail to reject our null hypothesis and conclude that there is material B wears better than material A with a p-value= 0.9957Next, we use the “bacteria” dataset in R and use it to answer the question: Did the drug treatment have a significant effect on the presence of the bacteria compared with the placebo?”We define the corresponding null and alternative hypothesis as follows:H0: pd ≤pp\n",
      "\n",
      "H1: pd > pp    where pd is the observed proportion of drug treatment and    pp is the observed proportion for the placebo treatment.Therefore, we have a one-side hypothesis problem.We obtain the sample sizes of drug and placebo using the code> drug<- length(which(bacteria$trt== \"drug\"))> drug[1] 62> placebo<- length(which(bacteria$trt== \"placebo\"))> placebo[1] 92We obtain the observed proportion of drug pd with effect on bacteria from the R code:> #Drug count with effect on bacteria> count2 <- length(which(bacteria$trt== \"drug\" & bacteria$y==\"y\" ))> count2[1] 44 Consequently, we obtain the observed proportion of placebo with effect on bacteria from:> #Placebo count with effect on bacteria> count1 <- length(which(bacteria$trt== \"placebo\" & bacteria$y==\"y\" ))> count1[1] 84Then we proceed to perform the test of proportions in R using the code:> test<- prop.test(x = c(44, 84), n = c(62, 92), alternative = \"greater\")> test \t2-sample test for equality of proportions with continuity correctiondata:  c(44, 84) out of c(62, 92)X-squared = 9.5151, df = 1, p-value = 0.999alternative hypothesis: greater95 percent confidence interval: -0.323288  1.000000sample estimates:   prop 1    prop 2 0.7096774 0.9130435 From the above results we obtain:the value of Pearson’s chi-squared test statistic = 9.5151a p-value (p-value= 0.999)95% confidence interval (-0.323288, 1.0000)the proportion of effective drugs in the two groups)Since the p-value= 0.999 is greater than the level of significance alpha= 0.05. We fail to reject the null hypothesis. Therefore, we can conclude that the drug treatment did not have a significant effect on the presence of the bacteria compared with the placebo with a p-value of 0.999Finally, we perform a F-test on the cats’ dataset in R to test for the variance of the body weight in male and female cats.We define our corresponding null and alternative hypothesizes as: H0:σ2m=σ2f\n",
      "\n",
      "H1: σ2m ≠ σ2f        where σ2m is the variance of males and σ2f    is the variance for female catsThen, we perform the F-test on the variance using the R function var.test as follows:> ftest <- var.test(cats$Bwt~cats$Sex , alternative=\"two.sided\", date=cats)> ftest\tF test to compare two variancesdata:  cats$Bwt by cats$SexF = 0.3435, num df = 46, denom df = 96, p-value = 0.0001157alternative hypothesis: true ratio of variances is not equal to 195 percent confidence interval: 0.2126277 0.5803475sample estimates:ratio of variances          0.3435015 From the above results, we obtain:the value of the F -test statistic (F= 0.3435)the degrees of the freedom of the F distribution of the test statistic (df=46)the p-value of the test. (p-value= 0.0001157)95% confidence interval for the ratio of the population variances (0.2126277, 0.5803475)the ratio of the sample variances = 0.3435015Since the p-value = 0.0001157 is less than the level of significance = 0.05, we reject the null hypothesis and conclude that there is no significant differences between the male and famales variances.Part BIn this part, we use the “Rubber” dataset in R package “MASS” to perform some multiple regression.First, we load the package and the dataset into R as follows:> library(MASS)> data(\"Rubber\")Next we use the str() to get a glimpse of the structure of our dataframe.> str(Rubber)'data.frame':\t30 obs. of  3 variables: $ loss: int  372 206 175 154 136 112 55 45 221 166 ... $ hard: int  45 55 61 66 71 71 81 86 53 60 ... $ tens: int  162 233 232 231 231 237 224 219 203 189 ...The dataset has 30 rows/ observations and 3 integer variables/ features. The variable contained in the model are: loss (the abrasion loss in gm/hr),  hard (hardness in `Shore’ units), and tens (tensile strength in kg/sq m).( J H Maindonald.(2008). Using R for Data Analysis and Graphics. Australian National University).We plot a multiple regression line using the response variable loss and the predictor variables hard and tens in R as follows:> model <- lm(loss~hard+tens, data=Rubber) > summary(model)Call:lm(formula = loss ~ hard + tens, data = Rubber)Residuals:    Min      1Q  Median      3Q     Max -79.385 -14.608   3.816  19.755  65.981 Coefficients:            Estimate Std. Error t value Pr(>|t|)    (Intercept) 885.1611    61.7516  14.334 3.84e-14 ***hard         -6.5708     0.5832 -11.267 1.03e-11 ***tens         -1.3743     0.1943  -7.073 1.32e-07 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 36.49 on 27 degrees of freedomMultiple R-squared:  0.8402,\tAdjusted R-squared:  0.8284 F-statistic:    71 on 2 and 27 DF,  p-value: 1.767e-11As seen, our multiple linear regression equation is:Loss= -79.385 - 6.5708*hard - 1.3743*tensWe also obtain the correlation coefficient (Multiple R-squared). This measures the proportion of variation in the data that is accounted for in the model. It measures how well the model fits the data. It tells us how strong the linear relationship is. It ranges from a value of 0 to 1. A value of 1 means a perfect positive relationship while a value of zero means no relationship at all. Our model returns a correlation coefficient of 0.84 which is a satisfactory indicator of a positive linear relationship in the model.The adjusted R-squared measures the variation of the dependent variables around the mean that are explained by the x-values. It tells how many points fall in the regression line. In our case, 82.8% of the dependent variables fit the model.Next, we diagnose our model to evaluate its efficacy.On the surface, we’d be interested if there is a linear relationship between the independent and dependent variables and the correlation coefficients between the variables to test the strength of association between the variables.We achieve these two on the same plot using the pairs.panel() function in “psych” package in R.We install the package, load it and then plot our pair plot as follows:> install.packages(\"psych\")> library(psych)> pairs.panels(Rubber[c(\"loss\", \"hard\", \"tens\")])The correlation coefficient ranges from -1 to 1. The sign of the correlation coefficient indicates the direction of the association. The magnitude of the correlation coefficient indicates the strength of the association. For example, the correlation coefficient of r=-0.30 between loss and tens suggest a weaker negative association between the two variables. This means there exists a weak negative linear relationship between loss and tens variables. This can also be seen in the scatter plot between the two (at bottom left corner). The same can also be said about the loss and hard variables.The correlation coefficient of r=-0.74 between loss and hard suggest a stronger negative association between the two variables. The same can be observed from their scatterplots. An increase in hardness of rubber leads to a decrease in abrasion (loss) of the rubber.Next, we consider the distribution of the residuals. The residuals are assumed to be normally distributed, have a constant variance (homoscedasticity) and are independent.We use the plot() function in R to give us the 4 diagnostic models;> plot(model)The Residuals vs Fitted plot is used to check the linear relationship assumptions.  A horizontal line, without distinct patterns is an indication of a perfect linear relationship. In our case, the relationship is nearly linear but some departure is observed at the lower and upper values.The Normal Q-Q plot is used to examine whether the residuals are normally distributed. Residuals are normally distributed if they fall on or near the straight dotted line.In our case, there is a significant departure from the dotted line particulary in the lower and upper values. This indicates non-normality of the errors. However, most the residual fall on and close to the dotted line. We can fairly assume normality of our residuals.The Scale-Location is used to check the homogeneity of variance of the residuals (homoscedasticity). A horizontal line with equally spread points is a good indication of homoscedasticity. From our plot, the variance of the residuals tends to be fairly linear except for the residuals for the upper values.Lastly, we check the influential values from our plot. Influential values are those whose absence would significantly change the regression equation. In regression, such a value is not always associated with the regression line.From our plot, the solid line corresponds to the regression with all the data, while the lower right dashed line corresponds to the regression with the point on the upper-left (28) removed.Clearly, that value has a huge influence on the regression.The same case with the influential value (14), whose exclusion would lead to the dotted regression line on the upper right.Finally, we install and load up the “oddbooks” dataset into R and check it’s basic structure  as follows:> install.packages(\"DAAG\")> library(DAAG)> str(oddbooks)'data.frame':\t12 obs. of  4 variables: $ thick  : int  14 15 18 23 24 25 28 28 29 30 ... $ height : num  30.5 29.1 27.5 23.2 21.6 23.5 19.7 19.8 17.3 22.8 ... $ breadth: num  23 20.5 18.5 15.2 14 15.5 12.6 12.6 10.5 15.4 ... $ weight : int  1075 940 625 400 550 600 450 450 300 690 ...The dataset contains 12 rows/ observations and 4 variables thick, height, breadth and weight all which are numeric.We proceed to plot a multiple regression model using weight as the response variable and the predictor variables height, weight and breadth.> model2<-lm(weight~thick+height+breadth, data=oddbooks)> summary(model2)Call:lm(formula = weight ~ thick + height + breadth, data = oddbooks)Residuals:    Min      1Q  Median      3Q     Max -156.60  -12.22   22.54   49.87   71.56 Coefficients:            Estimate Std. Error t value Pr(>|t|)  (Intercept) -441.651    720.770  -0.613   0.5571  thick          4.961     10.308   0.481   0.6432  height       -21.947     45.898  -0.478   0.6453  breadth       91.666     43.534   2.106   0.0683 .---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 87.47 on 8 degrees of freedomMultiple R-squared:  0.9089,\tAdjusted R-squared:  0.8747 F-statistic: 26.59 on 3 and 8 DF,  p-value: 0.0001635Our regression equation is:Weight= -441.651 + 4.961* thick – 21.947*height + 91.666* breadthFrom the output, we obtain the correlation coefficient (Multiple R-squared). This measures the proportion of variation in the data that is accounted for in the model. It measures how well the model fits the data. It tells us how strong the linear relationship is. It ranges from a value of 0 to 1. A value of 1 means a perfect positive relationship while a value of zero means no relationship at all. Our model returns a correlation coefficient of 0.9089 which is a strong indicator of a positive linear relationship in the model.The adjusted R-squared measures the variation of the dependent variables around the mean that are explained by the x-values. It tells how many points fall in the regression line. In our case, 87.47% of the dependent variables fit the model.Next, we’d be interested if there is a linear relationship between the independent and dependent variables and the correlation coefficients between the variables to test the strength of association between the variables.We achieve these two on the same plot using the pairs.panel() function in “psych” package in R as follows:> library(psych)> pairs.panels(oddbooks[c(\"weight\", \"thick\", \"height\",\"breadth\")])The correlation coefficient ranges from -1 to 1. The sign of the correlation coefficient indicates the direction of the association. The magnitude of the correlation coefficient indicates the strength of the association. For example, the correlation coefficient of r= 0.91 between height and weight suggest a strong negative association between the two variables. This means there exists a strong positive linear relationship between the variables. This can also be seen in the scatter plot between the two The same can also be said about the weight and breadth variables.The correlation coefficient of r=-0.94 between thickness and height suggest a strong negative association between the two variables. The same can be observed from their scatterplots. An increase in height of the books of leads to a decrease in thickness of the books. The same negative relationship can be seen between thickness and weight, height and breadth variables coefficient of correlation.Next, we consider the distribution of the residuals. The residuals are assumed to be normally distributed, have a constant variance (homoscedasticity) and are independent.We use the plot() function in R to give us the 4 diagnostic models;> plot(model2)The Residuals vs Fitted plot is used to check the linear relationship assumptions.  A horizontal line, without distinct patterns is an indication of a perfect linear relationship. In our case, the relationship is nearly linear but some departure is observed at the lower values.The Normal Q-Q plot is used to examine whether the residuals are normally distributed. Residuals are normally distributed if they fall on or near the straight dotted line.In our case, there is a significant departure from the dotted line particulary in the lower values. This indicates non-normality of the errors. This shows that our model is not to be relied upon.The Scale-Location is used to check the homogeneity of variance of the residuals (homoscedasticity). A horizontal line with equally spread points is a good indication of homoscedasticity. From our plot, the variance of the residuals is non-linear. Hence, we observe a lack of constant residual variance across the range of the predicted values.Errors are greater for some portions of the range than for others.A phenomenon known as Heteroscedasticity. This may suggest an incomplete model.Lastly, we check the influential values from our plot. Influential values are those whose absence would significantly change the regression equation. In regression, such a value is not always associated with the regression line.From our plot, the solid line corresponds to the regression with all the data, while the lower right dashed lines correspond to the regression with the point on the upper-left (10) removed.Clearly, that value has a huge influence on the regression and should not be omitted although it may be viewed as an outlier.ConclusionIn conclusion, while the all assumptions of a linear regression model are not met in reality. We must check if they are reasonable assumptions to work with. It’s a judgmental issue on the part of the Data Analyst to check if some twerks can be made before going forward confidently to use the regression model. If most/ none of the assumption are not met, a linear regression is clearly not a good fit and the Data Scientist may consider other models like the Generalized Linear Model which is an extension to the linear regression model to address the situation where it would not be applicable. In our inferential analysis of a dataset, Hypothesis testing is a handy tool that can be used to help validate an assumption being made about data relationships. The decision-making criteria have to be based on certain parameters of datasets. However, to reduce any bias in our decision making out of these hypotheses, techniques such as Bootstrapping and Resampling can be employedReferences1. J H Maindonald.(2008). Using R for Data Analysis and Graphics. Australian National University.2. R Documentation. (2018). Retrieved from:  https://www.statmethods.net/stats/rdiagnostics.html         \n"
     ]
    }
   ],
   "source": [
    "#Extracting text from Word File\n",
    "import docx\n",
    "from docx import Document\n",
    "#Creating a word file object\n",
    "doc=open(\"Hypothesis Testing.docx\", \"rb\")\n",
    "#Creating word reader object\n",
    "document=docx.Document(doc)\n",
    "#For loop that go through each paragraph in word document and appends the paragraph\n",
    "docu=\"\"\n",
    "for para in document.paragraphs:\n",
    "    docu += para.text\n",
    "\n",
    "print(docu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n"
     ]
    }
   ],
   "source": [
    "#Handling Strings\n",
    "String=\"I am exploring NLP\"\n",
    "print(String[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am learning NLP\n"
     ]
    }
   ],
   "source": [
    "String_2=String.replace('exploring','learning')\n",
    "print(String_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Searching for a substring in a string\n",
    "String=\"I am learning NLP\"\n",
    "f='learn'\n",
    "String.find(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping Text from Web\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from ipywidgets import FloatProgress\n",
    "from time import sleep\n",
    "from IPython.display import display\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='http://www.imdb.com/chart/top?ref_=nv_mv_250_6'\n",
    "result=requests.get(url)\n",
    "c=result.content\n",
    "soup=BeautifulSoup(c,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary=soup.find('div', {'class:article'})\n",
    "#Creating empty lists tto append the extracted data\n",
    "moviename=[]\n",
    "cast=[]\n",
    "description=[]\n",
    "rating=[]\n",
    "ratingoutof=[]\n",
    "year=[]\n",
    "genre=[]\n",
    "movielength=[]\n",
    "rot_audscore=[]\n",
    "rot_avgrating=[]\n",
    "rot_users=[]\n",
    "#Extracting the required data from the html soup \n",
    "rgx = re.compile('[%s]' % '()') \n",
    "f = FloatProgress(min=0, max=250) \n",
    "display(f) \n",
    "for row,i in zip(summary.find('table'). \n",
    "findAll('tr'),range(len(summary.find('table').findAll('trfor sitem in row.findAll('span',{'class':'secondaryInfo'}):        s = sitem.find(text=True)        year.append(rgx.sub(\", s))     for ritem in row.findAll('td',{'class':'ratingColumn imdbRating'}):        for iget in ritem.findAll('strong'):\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 11), match='Hello James'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Simple Chatbot\n",
    "import re\n",
    "r = r\"[^a-z]*([y]o|[h']?ello|ok|hey|(good[ ])?(morn[gin']{0,3}|\" r\"afternoon|even[gin']{0,3}))[\\s,;:]{1,3}([a-z]{1,20})\"\n",
    "re_greeting=re.compile(r, flags=re.IGNORECASE)\n",
    "re_greeting.match('Hello James')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Hello', None, None, 'James')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_greeting.match('Hello James').groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Good evening', 'Good ', 'evening', 'James')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_greeting.match('Good evening James Parker').groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 17), match=\"Good morn'n James\">"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_greeting.match(\"Good morn'n James\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello James\n",
      "Hi , How are you?\n"
     ]
    }
   ],
   "source": [
    "my_names=set(['james','jaymo','chatty','chatbot','bot','chatterbot'])\n",
    "curt_names=set(['hal','you','u'])\n",
    "greeter_name=\"\"\n",
    "match=re_greeting.match(input())\n",
    "if match:\n",
    "    at_name=match.groups()[-1]\n",
    "    if at_name in curt_names:\n",
    "        print(\"Good One.\")\n",
    "    elif at_name.lower() in my_names:\n",
    "        print(\"Hi {}, How are you?\".format(greeter_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
