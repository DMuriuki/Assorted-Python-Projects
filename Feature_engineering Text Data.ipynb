{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering Text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>New</th>\n",
       "      <th>York</th>\n",
       "      <th>city</th>\n",
       "      <th>love</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   I  New  York  city  love\n",
       "0  1    0     0     0     0\n",
       "1  0    0     0     0     1\n",
       "2  0    1     0     0     0\n",
       "3  0    0     1     0     0\n",
       "4  0    0     0     1     0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "Text=\"I love New York city\"\n",
    "pd.get_dummies(Text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'new': 4, 'york': 10, 'and': 0, 'will': 8, 'travel': 7, 'to': 6, 'next': 5, 'month': 2, 'with': 9, 'my': 3}\n"
     ]
    }
   ],
   "source": [
    "#Using CountVectorizer- taking frequency of word occurrence into account\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "Text=[\"I love New York and I will travel to New York next month with my love\"]\n",
    "vectorizer=CountVectorizer()\n",
    "#Tokenizing\n",
    "vectorizer.fit(Text)\n",
    "#Encoding text\n",
    "vector=vectorizer.transform(Text)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 1 1 2 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['I', 'love']), WordList(['love', 'New']), WordList(['New', 'York'])]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generating N-Grams\n",
    "Text=\"I love New York\"\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "TextBlob(Text).ngrams(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love new': 1, 'new york': 4, 'york and': 10, 'and will': 0, 'will travel': 8, 'travel to': 7, 'to new': 6, 'york next': 11, 'next month': 5, 'month with': 2, 'with my': 9, 'my love': 3}\n"
     ]
    }
   ],
   "source": [
    "# Generating Bi-grams based features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "Text=[\"I love New York and I will travel to New York next month with my love\"]\n",
    "vectorizer=CountVectorizer(ngram_range=(2,2))\n",
    "#tokenizing\n",
    "vectorizer.fit(Text)\n",
    "#encode text\n",
    "vector=vectorizer.transform(Text)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 2 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Co-occurrence matrix\n",
    "import numpy as np\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "def co_occurence_matrix(corpus):\n",
    "    vocab=set(corpus)\n",
    "    vocab=list(vocab)\n",
    "    vocab_to_index={word:i for i, word in enumarate(vocab)}\n",
    "    bi_grams=list(bigrams(corpus))\n",
    "    #Frequency distribution of bigrams\n",
    "    bigram_freq=nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "    #Intializing cooccurrence matrix\n",
    "    co_occurrence_matrix=np.zeros((len(vocab),len(vocab)))\n",
    "    #looping through the bigrmas taking the current and previous word and the number of occurrences of the bigram\n",
    "    for bigram in bigram_freq:\n",
    "        current=bigram[0][1]\n",
    "        previous=bigram[0][0]\n",
    "        count=bigram[1]\n",
    "        pos_current=vocab_to_index[current]\n",
    "        pos_previous=vocab_to_index[previous]\n",
    "        co_occurrence_matrix[pos_current][pos_previous]=count\n",
    "    co_occurrence_matrix=np.matrix(co_occurrence_matrix)\n",
    "    return co_occurrence_matrix,vocab_to_index\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'co_occurrence_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-37c74838678f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#Creating one list using many lists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmatrix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mco_occurrence_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mFinal_matrix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab_to_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab_to_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFinal_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'co_occurrence_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "texts=[['I','love', 'Boston', 'city'],['I', 'love', 'to', 'learn'],['AI', 'is', 'the','future'],['NLP', 'is', 'fun', 'to', 'learn']]\n",
    "#Creating one list using many lists\n",
    "merge=list(itertools.chain.from_iterable(texts))\n",
    "matrix=co_occurrence_matrix(merge)\n",
    "Final_matrix=pd.DataFrame(matrix[0],index=vocab_to_index, columns=vocab_to_index)\n",
    "print(Final_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.23570226  0.          0.47140452  0.47140452 -0.70710678\n",
      "   0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Hash Vectorizing- stores tokens as numerical indexes\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "Text=[\"I love New York and I will travel to New York next month with my love\"]\n",
    "vectorizer=HashingVectorizer(n_features=10)\n",
    "#Creating a hshing vector\n",
    "vector=vectorizer.transform(Text)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 7, 'boston': 1, 'city': 2, 'to': 10, 'learn': 6, 'ai': 0, 'is': 5, 'the': 9, 'future': 4, 'nlp': 8, 'fun': 3}\n"
     ]
    }
   ],
   "source": [
    "#Converting texts into features using TF-IDF\n",
    "text=['I love Boston city','I love to learn','AI is the future','NLP is fun to learn']\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer=TfidfVectorizer()\n",
    "vectorizer.fit(text)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.91629073 1.91629073 1.91629073 1.91629073 1.51082562\n",
      " 1.51082562 1.51082562 1.91629073 1.91629073 1.51082562]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
